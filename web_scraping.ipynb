{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6YYgYqwuUfB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "### importing needed packages\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "2NKL08zDIk7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### function to extract links for next pages and the pages' names\n",
        "def fetch_links(soup, startStr, tagName, nameClass, startInd = 0):\n",
        "  links = []\n",
        "  tags = soup.find_all(tagName, {'class':nameClass})\n",
        "  names = [tag.get_text() for tag in tags]\n",
        "  for link in soup.find_all('a', href=True):\n",
        "    string = link['href']\n",
        "    if string.startswith(startStr,startInd):\n",
        "      links.append(string)\n",
        "\n",
        "\n",
        "  return list(zip(names,links))\n",
        "\n",
        "\n",
        "def fetch_links_only(soup, startStr, startInd = 0):\n",
        "  links = []\n",
        "  for link in soup.find_all('a', href=True):\n",
        "    string = link['href']\n",
        "    if string.startswith(startStr,startInd):\n",
        "      links.append(string)\n",
        "\n",
        "\n",
        "  return links\n",
        "\n",
        "def fetch_poem_info(soup, startStr, startInd = 0):\n",
        "\n",
        "  for link in soup.find_all('a', href=True):\n",
        "    string = link['href']\n",
        "    if string.startswith(startStr,startInd):\n",
        "        return string\n",
        "\n",
        "## funtion to recreate full links for the next pages so we can extract their content\n",
        "def full_links(resultList):\n",
        "  links = []\n",
        "  for item in resultList:\n",
        "    links.append('https://www.aldiwan.net/'+item[1])\n",
        "\n",
        "  return links\n",
        "\n",
        "def full_links_poems(resultList):\n",
        "  links = []\n",
        "  for item in resultList:\n",
        "    links.append('https://www.aldiwan.net/'+item)\n",
        "\n",
        "  return links\n",
        "### funtion to create list of soups\n",
        "def get_soup(links):\n",
        "  soups = []\n",
        "  for URL in links:\n",
        "    page = requests.get(URL)\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "    soups.append(soup)\n",
        "\n",
        "  return soups\n"
      ],
      "metadata": {
        "id": "F61A5vi5HJhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### extract the main page\n",
        "URL = \"https://www.aldiwan.net/poets-antiquity\"\n",
        "page = requests.get(URL)\n",
        "soup1 = BeautifulSoup(page.content, \"html.parser\")"
      ],
      "metadata": {
        "id": "MyXL3C-TPces"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### get the links from the main page and create list of the full links\n",
        "page_1 = fetch_links(soup1, 'cat-poets', 'a', \"s-button\")\n",
        "links_1 = full_links(page_1)"
      ],
      "metadata": {
        "id": "GvlGqGDfHkLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links_1"
      ],
      "metadata": {
        "id": "AYM7MBfPkHoi",
        "outputId": "60308e5a-1500-4f10-d337-78046d27c670",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://www.aldiwan.net/cat-poets-pre-islamic-period',\n",
              " 'https://www.aldiwan.net/cat-poets-Islamic-era',\n",
              " 'https://www.aldiwan.net/cat-poets-abbasid-era',\n",
              " 'https://www.aldiwan.net/cat-poets-ayubi-era',\n",
              " 'https://www.aldiwan.net/cat-poets-ottoman-era',\n",
              " 'https://www.aldiwan.net/cat-poets-veteran',\n",
              " 'https://www.aldiwan.net/cat-poets-umayyad-era',\n",
              " 'https://www.aldiwan.net/cat-poets-andalusian-era',\n",
              " 'https://www.aldiwan.net/cat-poets-mamluk-era']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### get the soups of all links in the main page (each soup has the links of each era poets)\n",
        "era_soups = get_soup(links_1)"
      ],
      "metadata": {
        "id": "PAU5d-iADxRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def full_links(resultList):\n",
        "  links = []\n",
        "  for item in resultList:\n",
        "    links.append('https://www.aldiwan.net/'+item)\n",
        "\n",
        "  return links\n"
      ],
      "metadata": {
        "id": "Ezlxs1BPqtyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages = []\n",
        "for soup in era_soups[0:1]: #iterate over each soup of the era soup\n",
        "\n",
        "\n",
        "  pages = fetch_links_only(soup, 'cat-poet')[::2]"
      ],
      "metadata": {
        "id": "1_Fok2MdpR9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 0\n",
        "data = {}\n",
        "for soup in era_soups[0:1]: #iterate over each soup of the era soup\n",
        "  n+=1\n",
        "\n",
        "  soups = []\n",
        "  pages = fetch_links_only(soup, 'cat-poet')[::2]\n",
        "  names = [item[0] for item in fetch_links(soup, 'cat-poet', 'span', \"h3\")]\n",
        "  # page = fetch_links(soup, 'cat-poet', 'span', \"h3\") # this a list of tuples like this one: ('السمهري العلكي', 'cat-poet-alsmhry-alki')\n",
        "  links = full_links(pages) # this is poets' links\n",
        "  poets_soups = get_soup(links) #list of poets' soups\n",
        "  for i in range(len(poets_soups[:8])): #iterate over pages where each page is one poet's poems\n",
        "    page2 = fetch_links_only(poets_soups[i], 'poem')\n",
        "    name = names[i]\n",
        "    meta = poets_soups[i].find_all('h4', href=False)  ##############################################\n",
        "    links2 = full_links_poems(page2) # this is poems' links\n",
        "    poems_soups = get_soup(links2) #list of poems' soups\n",
        "    poems = []\n",
        "    # for soup in poems_soups:\n",
        "    #   links3 = fetch_links_only(soup, 'poem')\n",
        "\n",
        "    for link in links2:\n",
        "        page = requests.get(link)\n",
        "\n",
        "\n",
        "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "        results = soup.find(id=\"poem_content\")\n",
        "\n",
        "        # poem = results.prettify()\n",
        "        poem = results.get_text(',')   ###################################################\n",
        "        topic = fetch_poem_info(soup, 'Poems-Topics-')\n",
        "        sea = fetch_poem_info(soup, 'sea-')\n",
        "        typ = fetch_poem_info(soup, 'Type-')\n",
        "        qafya = fetch_poem_info(soup, 'q-')\n",
        "\n",
        "        poems.append((poem, topic, typ, sea, qafya))\n",
        "\n",
        "\n",
        "    data.update({name:(n, meta, poems[::2])}) ############################################\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1NbdACoYvXjn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}